

%%%%%%%%%%%%%% 特徴量工学の説明 %%%%%%%%%%%%
\section{\mc 提案手法の狙い}

\subsection{従来手法の問題点}
典型的な運動想起型BCIは、運動想起に関連しているEEGを取り出すための前処理\(\cal H(\cdot)\)
によってEEGの生データ\(X\)から\(\hat X\)を獲得することが一般的である。
\begin{equation}
    \hat X = {\cal H}(X)
    \label{eq:bandpass}
\end{equation}
次に、運動想起に関連している空間的な情報を抽出する処理を\(f(\cdot)\)を適用し、
特徴量\(Z\)を取り出す。
\begin{equation}
    Z = f(\hat X)
    \label{eq:spatfilter}
\end{equation}
続いて\(Z\)に対して、
運動想起部位\(Y\)を出力する分類器\(g(\cdot)\)を準備することで、運動想起BCIが構成されている。
\begin{equation}
    Y = g(Z)
    \label{eq:classifier}
\end{equation}
従って、BCIはEEG\(X\)を引数とした合成関数という形式を取る。
\begin{equation}
    Y = (g\circ f \circ {\cal H})(X)
    \label{eq:bci_gosei}
\end{equation}
実際に合成関数としてどのようなものが選択されるか、
EEGがどのように測定されるかはタスクに依存するが、
典型的なCSPを用いたBCIでは\(X\)を運動想起時のEEG、\(\cal H\)をバンドパスフィルタ、
\(f\)をCSP、\(g\)をLDAやSVMとして各々個別に設計する。

一方で時間周波数解析に基づくBCIでも特徴抽出として
何らかの時間周波数解析\(h(\cdot)\)が関数内に挿入され、
\begin{equation}
    Y = (g\circ h \circ f \circ {\cal H})(X)
    \label{eq:bci_gosei2}
\end{equation}
という形式で表せる。この時、\(\cal H\)や\(f\)はERDを検出するための
神経科学的な知見に基づいた設計がなされる場合もあれば、
機械学習の手法が用いられる場合もある。
更に時間周波数解析によって得られるパワースペクトログラムに対して
非負値行列分解などを用いて特徴量を抽出する試みもある\cite{kNMF,kNMF2}。
この場合も行列分解による変換を\(a(\cdot)\)と置けば
\begin{equation}
    Y = (g\circ a\circ h\circ f\circ {\cal H})(X)
    \label{eq:bci_gosei3}
\end{equation}
と表され、形式上は合成関数である。
それぞれの関数の役割を明示しなければ、BCIは単に以下の合成関数である。
\begin{equation}
    Y = (f_K\circ \cdots \circ f_1)(X)
    \label{eq:bci_gosei4}
\end{equation}
すなわちBCIは設計を終えた時には何らかの合成関数として構築されている。
実際に設計を行う際には各々の合成関数に対して関数族を仮定し、
学習やヒューリスティクスによって関数を決定し、最後に合成する。
しかしこのとき、BCIのデータの流れから明らかに関数\(f_i\)を設計するためには
\(f_{i-1}\)の設計が終了していなければならない。

更に、商業利用が目される中で従来の方法によって設計が個別に行われることを想定した場合には
BCIの設計者の負担と不足が見込まれる。

\begin{figure}[pt]
    \centering
    \includegraphics[width=13cm]{images/jurai.png}
    \caption{従来のBCI設計法}
    \label{fig:hikaku1}
\end{figure}
\begin{figure}[pt]
    \centering
    \includegraphics[width=13cm]{images/teian.png}
    \caption{提案するBCI設計法}
    \label{fig:hikaku2}
\end{figure}

\subsection{End to End学習}
End to End学習による狙いはEEGの個別の解析を排除し、
BCIの設計を自動化することである。
% 本研究では音声認識や画像認識の分野で高い性能を発揮している
% ニューラルネットワークに着目する。
% ニューラルネットワークは学習時の計算量が膨大であることから、
% 長い間研究が滞っていたが、近年の計算機の発展により応用研究に用いられるようになり、
% 人間の設計した分類器を凌駕する性能の高さから注目が集まっている。

本研究ではBCIが元来合成関数として構築されてきたことを考慮し、
適切なニューラルネットワークを構成することでEnd to End学習を目指した。
ニューラルネットワークの実体は巨大な合成関数であり、
事実上単なるパラメトリックな数理モデルである。
誤差逆伝播学習によって原理上極めて
深い合成関数の形式であっても学習が可能であるため、
問題に応じてネットワーク構造の設計を適切に行うことで、
特徴量抽出と分類を同時に達成できる可能性がある。
更にその学習のアルゴリズム自体は極めて単純であるが効果的に働き\cite{CheapLearning}、
再学習可能であるため有用なニューラルネットワークの構造が発見された場合にその再利用性が高い。
加えて、ニューラルネットワークの深層構造によって高い性能が引き出されること\cite{DeepvsShallow}や、
モデルの複雑さと比較して最適化問題の目的関数は病理的ではない、
あるいは病理的な形状を回避できることが示唆されるようになった\cite{ディープローカルミニマム}。

更にニューラルネットワークは学習済のパラメータを初期値に再学習が可能である。
また、学習済のパラメータの一部を定数として扱うことで、一種の特徴抽出器として転用（転移学習）することもできる。
この性質を利用することでEnd to Endで多数の人間のEEGを学習させておき、
実際にBCIを利用する際には個別用に転移学習という形で簡易に設計が済まされる。
従来の設計方法との違いの概念図を図\ref{fig:hikaku1}、\ref{fig:hikaku2}に示す。



初めに提案手法の大部分を占めるニューラルネットワークについて説明する。
ニューラルネットワークは近年、``深層学習''あるいは``ディープラーニング''の名の元に、
再注目を浴びている学習モデルである。ニューラルネットワークは元々脳の神経活動を模倣したモデルとして
提案されたが、現在では学習の力学特性や統計物理学の知見など数理的な解析が行われている\cite{ディープローカルミニマム,DeepvsShallow}。
あるいはヒューリスティクスによる発展が著しい。
従って、本論文では単にある特定の形式で表される数理モデルとしてニューラルネットワークを扱う。

% ニューラルネットワークはパラメータ\(W\)を持つ層\(Layer(\cdot,W)\)と
% パラメータを持たない変換\(f(\cdot)\)によって構成されている。
% 層と変換を用いて作られたパラメトリックな合成関数をニューラルネットワークと呼ぶ。
% ニューラルネットワークを構成することで、BCIの候補となる関数族を決めることができるが、
% 裏を返すと、BCIとしての振る舞いができるようなニューラルネットワークの構成を選ばなければ、
% 学習を長時間続けたとしても良い結果は得られない。
% \subsection{\mc モデル構築と学習}
% ニューラルネットワークは

\section{\mc ニューラルネットワーク}
\subsection{\mc ニューラルネットワークに用いられる層}
\subsubsection{\rm Linear\mc 層}
ニューラルネットワークで最も基本的な層はLinear層である。
Linear層は入力としてベクトル\(x \in \mathbb R^D\)、パラメータとして\(W \in \mathbb R^{d\times D}\)と\(b \in \mathbb R^d\)を有した
関数\(L(x,W) = Wx + b\)である。
% \begin{equation}
%     L(x,W) = \{Wx + b \mid W \in \mathbb R^{d\times D}, b \in \mathbb R^d\}
% \end{equation}
% Linear層は入力\(x\in R^D\)を\(d\)次元ベクトルへアフィン変換する。
% ここでLinear層の出力を\(y = Wx+b\)を成分表示をすると、
具体的には入力を\(x=(x_1,\cdots,x_D)^T\)に対して出力\(y=(y_1,\cdots,y_d)^T\)を以下で表すことができる。
\begin{equation}
    y_j = \sum_{i=1}^D  w_{j,i}x_i + b_j 
\end{equation}
ここに\(w_{j,i}\)は行列\(W\)の\((j,i)\)成分、\(b_j\)はベクトル\(b\)の\(j\)番目の成分である。
% 以下、層のパラメトリックな関数による記述と出力の成分表示にいずれの表記も場面に応じて使い分ける。

% \subsubsection{\rm Bilinear\mc 層}
% Bilinear層は入力として\(x^{(1)}\in \mathbb R^{D_1}\)と\(x^{(2)}\in \mathbb R^{D_2}\)の
% 2つを受け取り、1つの出力\(y \in \mathbb R^d\)を返す。
% \begin{equation}
%     y_k = \sum_{j=1}^{D_1}\sum_{i=1}^{D_2}w_{j,i,k}x_j^{(1)}x_i^{(2)} + b_k
% \end{equation}
% ここでn階のテンソル\(A\in \mathbb R^{D_1,\cdots ,D_n}\)の\((d_1,\cdots,d_n)\)成分を\(a_{d_1,\cdots,d_n}\)と表記している。
% パラメータは\(W\in \mathbb R^{D_1\times D_2 \times K}\)と\(b \in \mathbb R^K\)である。

\subsubsection{\rm Convolution\mc 層}
Convolution層はパラメータとして\(H \in \mathbb R^{P\times Q\times C\times L}\)と\(B \in \mathbb R^{I\times J\times L}\)を有し、
入力を\(X \in \mathbb R^{I\times J\times C}\)として、出力\(Y \in \mathbb R^{I'\times J'\times L}\)は以下で表される。
\begin{equation}
    y_{i,j,l} = \sum_{c=1}^C \sum_{p=0}^{P-1} \sum_{q=0}^{Q-1}x_{i+p,j+q,c}h_{p,q,c,l}+b_{i,j,l}
    \label{eq:convlayer}
\end{equation}
ここでもn階のテンソル\(A\in \mathbb R^{D_1,\cdots ,D_n}\)の\((d_1,\cdots,d_n)\)成分を\(a_{d_1,\cdots,d_n}\)と表記している。
出力テンソルの大きさを決める\(I',J'\)は\(I,J\)と\(P,Q\)に依存し、それぞれ\(I'=P-I+1\)と\(J'=Q-J+1\)となる。
しかし、Convolution層は実応用ではstrideやpadding、dilationなどのテクニックによって更に複雑な動作をする。
これらのテクニックはそれぞれハイパーパラメータを持ち、\(I',J'\)はその全てに依存する。
% Convolution層については\(X*H=\sum_{p=0}^{P-1} \sum_{q=0}^{Q-1}x_{i+p,j+q,c}h_{p,q,c,l}\)と置き、(\ref{eq:convlayer})を以下の関数のように略記する。
% \begin{equation}
%     C(X,H) = \sum_{c} X*H + B
%     \label{eq:convlayer2}
% \end{equation}

\subsubsection{\rm Recurrent\mc 層}
Recurrent層は入力\(X = (x_1,\cdots,x_T)\in \mathbb R^{D \times T}\)、
出力\(Y = (y_1,\cdots,y_T)\in \mathbb R^{d \times T}\)として以下で表される。
\begin{equation}
    y_t = {\rm tanh}(W_{in}x_t + W_{out}y_{t-1} + b) 
    \label{eq:reccurent}
\end{equation}
ここにパラメータは\(W_{in} \in \mathbb R^{d \times D}\)と\(W_{out} \in \mathbb R^{d \times d}\)ならびに、\(b \in \mathbb R^d \)である。
元々系列データへの応用のために考案されたため、
\(D\)次元のベクトル\(x_t \in \mathbb R^D\)が時間変化していくような場合を想定して
(\ref{eq:reccurent})と定式化される。
しかし、実際には入力は画像のような静的なデータでも構わなく、
画像を左から右に走査するような働きを担う。
\(\rm tanh(\cdot)\)は双曲線正接関数であるが、ニューラルネットワークの活性化関数として広く持ちいられている。

\subsubsection{\rm LSTM\mc 層}
LSTMはLong Short-Term Memoryの略であり、
LSTM層はReccurent層が長期的な系列データを上手く学習できない問題を解決した\cite{LSTM}。
LSTM層は入力\(X = (x_1,\cdots,x_T)\in \mathbb R^{D \times T}\)、
出力\(Y = (y_1,\cdots,y_T)\in \mathbb R^{d \times T}\)として以下で表される。
\begin{eqnarray}
    i_t &=& \sigma(W_{ii}x_t + W_{hi} h_{t-1} + b_i) \\
    f_t &=& \sigma(W_{if}x_t + W_{hf} h_{t-1} + b_f) \\
    o_t &=& \sigma(W_{io}x_t + W_{ho} h_{t-1} + b_o) \\
    g_t &=& {\rm tanh}(W_{ig}x_t + W_{hg} h_{t-1} + b_g) \\
    c_t &=& f_t \odot c_{t-1} + i_t \odot g_t \\
    y_t &=& o_t \odot {\rm tanh}(c_t)
    \label{eq:LSTM}
\end{eqnarray}
ここに\(\odot\)は同じ次元のベクトルの要素ごとの積である。
パラメータは\(W_{ii},W_{if},W_{io},W_{ig} \in \mathbb R^{d\times D}\)と、
\(W_{hi},W_{hf},W_{ho},W_{hg} \in \mathbb R^{d\times d}\)および、
\(b_{i},b_{f},b_{o},b_{g} \in \mathbb R^d\)である。
Recurrent層同様に、系列データを想定した定式化がなされているが、実際には入力が画像でも構わない。
\(\sigma(\cdot)\)はシグモイド関数であり(\ref{eq:logisticmodel})と同じものである。
ここでのシグモイド関数は、値域が\((0,1)\)であるために用いられている。

\(i_t,f_t,o_t,g_t\)は現在の入力\(x_t\)と過去の出力\(y_{t-1}\)を引数に取り、
それぞれ個別のパラメータを用いて値が算出される。
\(c_t\)はMemory Cellと呼ばれ、過去の情報を層の内部に記憶する役割を担っている。
\(f_t \in (0,1)^d \)を乗じることで、過去の情報\(c_{t-1}\)を一定の割合のみ保持し（あるいは忘却し）、
\(i_t\)に対して\(g_t \in (-1,1)^d\)を乗じた値を新たに加算して保持する。
\(y_t\)は内部情報である\(c_t\)を\((-1,1)^d\)にスケーリングしたベクトルと\(o_t\)との要素積によって算出される。
すなわち、\(o_t\)の値が内部情報\(c_t\)に応じて大きさと正負が調整されてLSTM層の出力となる。

\subsection{\mc 活性化関数}
ここではパラメータを持たない変換について述べる。
\subsubsection{\mc シグモイド関数}
シグモイド関数\(\sigma(x)\)は以下で定義される。
\begin{equation}
    \sigma(z) = \frac{1}{1+\exp(-z)}
\end{equation}

ニューラルネットワークは当初、神経活動の数理モデルとしての一面を有しており、
入力値がある閾値を超えると発火するというモデルを表現するためにステップ関数が利用されていた。
シグモイド関数は、ステップ関数を微分可能な形で表現することを動機に考案された。

\subsubsection{\mc 双曲線正接関数}
双曲線正接関数\({\rm tanh}(z)\)は以下で定義される。
\begin{equation}
    {\rm tanh}(z) = \frac{\exp(z) - \exp(-z)}{\exp(z)+\exp(-z)}
\end{equation}
先に述べたシグモイド関数とは\(\sigma(z)=({\rm tanh}(x/2) + 1)/2\)という関係にあり、
負の値を取ることもできるため状況に応じてシグモイド関数の代わりに用いられる。

\subsubsection{\mc ランプ関数}
ランプ関数はニューラルネットワークではRectified Linear Unit(ReLU)関数と呼ばれ、以下で表される。
\begin{equation}
    {\rm ReLU}(z) = \max(z,0)
\end{equation}
入力が正で\(-1\)の勾配を持ち、その他の場合は勾配は\(0\)となる。
この性質がニューラルネットワークの勾配消失問題を大きく改善したとされる。

\subsubsection{\rm leaklyReLU\mc 関数}
ReLU関数が入力が負の場合にも値を有するように提案された。
\begin{equation}
    {\rm leaklyReLU}(z) = \max(z,0) + \alpha \min(z,0)
\end{equation}
入力が正で\(-1\)の勾配を持ち、その他の場合は勾配は\(\alpha\)でハイパーパラメータである。

\subsubsection{\rm elu\mc 関数}
ReLU関数を全域で微分可能にするように調整された関数。
\begin{equation}
    {\rm elu}(z) = \max(z,0) + \min(\alpha(\exp(z)-1), 0)
\end{equation}
図\ref{fig:activation}はここまでに述べた5つの活性化関数の入出力を表した図である。
どのような場面でどの活性化関数が有効であるか、理論的な根拠は確認されておらず、
試行錯誤的に決定される。
\begin{figure}
    \centering
    \includegraphics[width=13cm]{images/activations.png}
    \caption{5つの活性化関数}
    \label{fig:activation}
\end{figure}

\subsubsection{\mc ソフトマックス関数}
ソフトマックス関数は一般的に、分類問題におけるニューラルネットワークの出力層に用いられる。
ベクトルの入力を受け取りベクトルの出力を行うが、以下に要素毎の計算を示す。
\begin{equation}
   {\rm softmax}_i(z) = \frac{\exp(z_i)}{\sum_j \exp(z_j)} 
\end{equation}
ここで、\(z_i\)はベクトル\(z\)の\(i\)番目の要素である。
定義から明らかに、値域は\((0,1)\)に制限され、出力の要素の総和は\(1\)である。

\subsection{\mc ニューラルネットワークの設計と学習}
ニューラルネットワークはパラメータを持つ関数とパラメータを持たない関数の
合成関数を定義することにより設計される。
パラメータを持つ層が入出力関係を柔軟に調整する役割を担い、
活性化関数によって非線形性を与えることが可能となる。

設計されたニューラルネットワークを引数とする損失関数(汎関数)を
定義し、損失関数の最小化問題を解くことで学習を実行する。
この際、損失関数はパラメータに関して微分可能なように設計することで
誤差逆伝搬法による学習が可能となる。。

クラス分類においては多くの場合にソフトマックス交差エントロピーを用いる。
ニューラルネットワークは膨大なパラメータを有するため、
学習させる入出力関係に対して冗長である場合がほとんどであり、
過学習を抑制する手段を同時に検討しなければならない。
機械学習で基本的な正則化に加え、ニューラルネットワークの過学習抑制方法としては
ドロップアウトやバッチ正規化、ノイズの入力が用いられる。

% \subsection{\mc 過学習抑制手法}
% \subsubsection{\mc ドロップアウト}
% ドロップアウトは過学習抑制のために考案された手法であり、以下で表される。
% \begin{eqnarray}
%     {\rm dropout}(z,p) & = & z \odot b \\
%     b  & \sim & {\rm Bern}(p)
% \end{eqnarray}
% \({\rm Bern}(p)\)は\(p\)の確率で\(1\)を出力し、\(1-p\)の確率で\(0\)を出力するベルヌーイ分布である。
% 従って、ドロップアウトは入力\(z\)の各要素は\(1-p\)の確率で値を\(0\)に変更する。
% ドロップアウトは入力が行われる度に毎回変更されるため、
% 学習時にはドロップアウトされなかった一部の要素に関わるパラメータのみが更新される。
% テスト時にはドロップアウトを行わず、代わりに\(zp\)を出力する。
% あるいはドロップアウトを用いたまま複数回サンプリングを行い、加算平均を出力とする。

% パラメータを持つ全ての層の後にドロップアウトを適用した場合は、
% ニューラルネットワークの学習がある仮定の下での変分ベイズ学習に厳密に一致する。
% また、テスト時に複数回のサンプリングによる出力を行った場合、ベイズ予測分布に一致する\cite{ベイズドロップアウト}。
% ただし、ベイズの解釈としては出力が確率的に\(0\)になるのではなく、学習パラメータが確率的に\(0\)になった結果、
% 出力が\(0\)になると考えなければならない。

% \subsubsection{\mc バッチ正規化}